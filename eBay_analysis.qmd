---
title: "eBay Auction Analysis"
author: "MISY 441-172"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
execute:
  warning: false
  message: false
jupyter: python3
---

## Business Problem

The file `eBayAuctions.csv` contains information on 1972 auctions transacted on eBay.com during May-June 2004. The goal is to use these data to build a model that will distinguish competitive auctions from noncompetitive ones.

A **competitive auction** is defined as an auction with at least two bids placed on the item being auctioned.

## Part a: Import Packages and Load Data

```{python}
import numpy as np
import pandas as pd
import sklearn as sk
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
ebay_df = pd.read_csv('eBayAuctions.csv')

print(f"Dataset loaded: {ebay_df.shape[0]} rows, {ebay_df.shape[1]} columns")
print("\nFirst 5 rows of the dataset:")
ebay_df.head()
```

## Part b: Observe Variables in the Dataset

```{python}
print("--- Data Types ---")
print(ebay_df.dtypes)
```

```{python}
print("--- Summary Statistics ---")
ebay_df.describe()
```

```{python}
print("--- Info about the dataset ---")
ebay_df.info()
```

## Part c: Explore Categorical Variables vs Competitive Auctions

### Category Analysis

```{python}
print("--- Competitive Auctions by Category ---")
category_competitive = ebay_df.groupby('Category')['Competitive'].agg(['mean', 'count'])
category_competitive = category_competitive.sort_values('mean', ascending=False)
print(category_competitive)
print(f"\nCategory with MOST competitive auctions: {category_competitive['mean'].idxmax()} "
      f"({category_competitive['mean'].max():.2%} competitive rate)")
```

### Currency Analysis

```{python}
print("--- Competitive Auctions by Currency ---")
currency_competitive = ebay_df.groupby('currency')['Competitive'].agg(['mean', 'count'])
currency_competitive
```

### End Day Analysis

```{python}
print("--- Competitive Auctions by End Day ---")
endday_competitive = ebay_df.groupby('endDay')['Competitive'].agg(['mean', 'count'])
endday_competitive = endday_competitive.sort_values('mean', ascending=False)
print(endday_competitive)
print(f"\nDay with MOST competitive auctions: {endday_competitive['mean'].idxmax()} "
      f"({endday_competitive['mean'].max():.2%} competitive rate)")
```

### Duration Analysis

```{python}
print("--- Competitive Auctions by Duration ---")
duration_competitive = ebay_df.groupby('Duration')['Competitive'].agg(['mean', 'count'])
duration_competitive
```

## Part d: Class Balance Analysis

What percentage of auctions are competitive? Is there an imbalance problem?

```{python}
competitive_counts = ebay_df.groupby('Competitive')['Competitive'].agg(['count'])
print("--- Count of Competitive vs Non-Competitive Auctions ---")
print(competitive_counts)

total = competitive_counts['count'].sum()
competitive_pct = competitive_counts.loc[1, 'count'] / total * 100
non_competitive_pct = competitive_counts.loc[0, 'count'] / total * 100

print(f"\nCompetitive auctions: {competitive_pct:.2f}%")
print(f"Non-competitive auctions: {non_competitive_pct:.2f}%")

if abs(competitive_pct - 50) > 10:
    print(f"\n⚠️  There appears to be a CLASS IMBALANCE issue.")
    print(f"   The classes are not evenly distributed ({competitive_pct:.1f}% vs {non_competitive_pct:.1f}%)")
else:
    print("\n✓ The classes are relatively balanced.")
```

```{python}
# Violin plot: Distribution of OpenPrice by Competitive Status (outliers removed)
# Remove outliers using IQR method
Q1 = ebay_df['OpenPrice'].quantile(0.25)
Q3 = ebay_df['OpenPrice'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter data to remove outliers
ebay_filtered = ebay_df[(ebay_df['OpenPrice'] >= lower_bound) & (ebay_df['OpenPrice'] <= upper_bound)].copy()
print(f"Removed {len(ebay_df) - len(ebay_filtered)} outliers from {len(ebay_df)} records")

# Create labels for the filtered data
ebay_filtered['Competitive_Label'] = ebay_filtered['Competitive'].map({0: 'Non-Competitive', 1: 'Competitive'})

plt.figure(figsize=(10, 6))
sns.violinplot(x='Competitive_Label', y='OpenPrice', data=ebay_filtered, 
               hue='Competitive_Label', palette=['#E74C3C', '#27AE60'], inner='box', legend=False)
plt.xlabel('Auction Type', fontsize=12)
plt.ylabel('Opening Price ($)', fontsize=12)
plt.title('Distribution of Opening Price: Competitive vs Non-Competitive Auctions\n(Outliers Removed)', fontsize=14)
plt.tight_layout()
plt.show()
```

## Part e: Converting Variables to Category Type

```{python}
# Create a copy for modeling
ebay_model = ebay_df.copy()

# Convert to category type
ebay_model['currency'] = ebay_model['currency'].astype('category')
ebay_model['endDay'] = ebay_model['endDay'].astype('category')
ebay_model['Category'] = ebay_model['Category'].astype('category')
ebay_model['Competitive'] = ebay_model['Competitive'].astype('category')

print("Variable types after conversion:")
print(ebay_model.dtypes)
```

## Part f: Creating Dummy Variables

```{python}
# Create dummy variables (dtype=int for statsmodels compatibility)
ebay_model = pd.get_dummies(ebay_model, prefix_sep='_', drop_first=False, dtype=int)

print(f"Shape after creating dummies: {ebay_model.shape}")
print("\nColumns after creating dummies:")
print(ebay_model.columns.tolist())
```

```{python}
# Drop reference categories (first category for each variable)
# Category_Automotive, currency_EUR, endDay_Mon
columns_to_drop = ['Category_Automotive', 'currency_EUR', 'endDay_Mon', 
                   'Competitive_0']  # Also drop one of the Competitive dummies

# Check which columns exist before dropping
existing_cols = [col for col in columns_to_drop if col in ebay_model.columns]
ebay_model.drop(columns=existing_cols, inplace=True)

# Rename Competitive_1 to Competitive
if 'Competitive_1' in ebay_model.columns:
    ebay_model.rename(columns={'Competitive_1': 'Competitive'}, inplace=True)

print(f"Shape after dropping reference categories: {ebay_model.shape}")
print("\nFinal columns for modeling:")
print(ebay_model.columns.tolist())
```

## Part g: Removing Outliers

```{python}
# Remove outliers using IQR method for numerical columns
numerical_cols = ['OpenPrice', 'ClosePrice', 'sellerRating']

print(f"Dataset size before removing outliers: {len(ebay_model)}")

# Create a mask for non-outliers
outlier_mask = pd.Series([True] * len(ebay_model))

for col in numerical_cols:
    Q1 = ebay_model[col].quantile(0.25)
    Q3 = ebay_model[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    col_mask = (ebay_model[col] >= lower_bound) & (ebay_model[col] <= upper_bound)
    outliers_in_col = (~col_mask).sum()
    print(f"  {col}: {outliers_in_col} outliers (range: {lower_bound:.2f} to {upper_bound:.2f})")
    
    outlier_mask = outlier_mask & col_mask

# Apply the mask to remove outliers
ebay_model = ebay_model[outlier_mask].copy()

print(f"\nDataset size after removing outliers: {len(ebay_model)}")
print(f"Total records removed: {(~outlier_mask).sum()}")
```

## Part h: Data Partitioning (60% Train / 40% Test)

```{python}
# Define X (features) and y (target)
# Convert y to int to ensure compatibility with statsmodels
y = ebay_model['Competitive'].astype(int)
X = ebay_model.drop(columns=['Competitive'])

# Split the data with random_state=202
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.40, random_state=202
)

print(f"Training set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
```

## Modeling Part 1: Logistic Regression with ALL Variables

```{python}
# Using statsmodels for detailed output
X_train_const = sm.add_constant(X_train)
X_test_const = sm.add_constant(X_test)

# Fit logistic regression using 'bfgs' method to avoid singular matrix issues
logit_model = sm.Logit(y_train, X_train_const)
result = logit_model.fit(method='bfgs', disp=0)

print("--- Logistic Regression Summary (with all variables including ClosePrice) ---")
print(result.summary())
```

```{python}
# Using sklearn for predictions
lr_model = LogisticRegression(max_iter=1000, random_state=202)
lr_model.fit(X_train, y_train)
```

## Modeling Part 2: Confusion Matrix and Accuracy

```{python}
# Predictions on test set
y_pred = lr_model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("--- Confusion Matrix (Validation Data) ---")
print(f"                 Predicted")
print(f"                 0       1")
print(f"Actual 0      {cm[0,0]:4d}    {cm[0,1]:4d}")
print(f"       1      {cm[1,0]:4d}    {cm[1,1]:4d}")

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
```

```{python}
# Confusion Matrix Plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Non-Competitive (0)', 'Competitive (1)'],
            yticklabels=['Non-Competitive (0)', 'Competitive (1)'],
            annot_kws={'size': 14})
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('Actual', fontsize=12)
plt.title('Confusion Matrix (With ClosePrice)', fontsize=14)
plt.tight_layout()
plt.show()
```

```{python}
# Classification Report
print("--- Classification Report ---")
print(classification_report(y_test, y_pred))
```

## Modeling Part 3: Challenge of Using ClosePrice

**ANSWER:** The challenge of using ClosePrice to predict if an auction will be competitive is that ClosePrice is **NOT AVAILABLE** at the time of prediction.

### Key Issues:

1. **DATA LEAKAGE:** ClosePrice is determined AFTER the auction ends, which means it is influenced by whether the auction was competitive or not. Using it as a predictor creates data leakage - we're using information that wouldn't be available when making predictions on new auctions.

2. **CAUSALITY:** Higher close prices often result FROM competitive bidding. This reverses the causal relationship - the outcome (competitive auction) causes the predictor (higher close price), not the other way around.

3. **PRACTICAL UTILITY:** In a real-world scenario, we want to predict whether an auction WILL BE competitive before it happens. ClosePrice is only known after the auction concludes, making it useless for prospective predictions.

This is why we need to build a model WITHOUT ClosePrice for practical use.

## Modeling Part 4: Model WITHOUT ClosePrice

```{python}
# Remove ClosePrice from features
X_no_close = X.drop(columns=['ClosePrice'])

# Split again with same random state
X_train_nc, X_test_nc, y_train_nc, y_test_nc = train_test_split(
    X_no_close, y, test_size=0.40, random_state=202
)

# Fit new model without ClosePrice
lr_model_nc = LogisticRegression(max_iter=1000, random_state=202)
lr_model_nc.fit(X_train_nc, y_train_nc)

# Predictions
y_pred_nc = lr_model_nc.predict(X_test_nc)
```

```{python}
# Confusion Matrix
cm_nc = confusion_matrix(y_test_nc, y_pred_nc)
print("--- Confusion Matrix WITHOUT ClosePrice ---")
print(f"                 Predicted")
print(f"                 0       1")
print(f"Actual 0      {cm_nc[0,0]:4d}    {cm_nc[0,1]:4d}")
print(f"       1      {cm_nc[1,0]:4d}    {cm_nc[1,1]:4d}")

# New Accuracy
accuracy_nc = accuracy_score(y_test_nc, y_pred_nc)
print(f"\nAccuracy WITHOUT ClosePrice: {accuracy_nc:.4f} ({accuracy_nc*100:.2f}%)")
```

```{python}
# Confusion Matrix Plot (Without ClosePrice)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_nc, annot=True, fmt='d', cmap='Oranges', 
            xticklabels=['Non-Competitive (0)', 'Competitive (1)'],
            yticklabels=['Non-Competitive (0)', 'Competitive (1)'],
            annot_kws={'size': 14})
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('Actual', fontsize=12)
plt.title('Confusion Matrix (Without ClosePrice)', fontsize=14)
plt.tight_layout()
plt.show()
```

```{python}
# Change in accuracy
accuracy_change = accuracy_nc - accuracy
print(f"--- Comparison ---")
print(f"Accuracy WITH ClosePrice:    {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Accuracy WITHOUT ClosePrice: {accuracy_nc:.4f} ({accuracy_nc*100:.2f}%)")
print(f"Change in Accuracy:          {accuracy_change:.4f} ({accuracy_change*100:.2f}%)")

if accuracy_change < 0:
    print(f"\n⚠️  Accuracy DECREASED by {abs(accuracy_change)*100:.2f}% after removing ClosePrice.")
    print("   This confirms that ClosePrice was contributing significantly to the model,")
    print("   but for practical predictions, the model without ClosePrice is more appropriate.")
else:
    print(f"\n✓ Accuracy changed by {accuracy_change*100:.2f}% after removing ClosePrice.")
```

```{python}
# Classification Report for model without ClosePrice
print("--- Classification Report (Without ClosePrice) ---")
print(classification_report(y_test_nc, y_pred_nc))
```

```{python}
# Statsmodels summary for the model without ClosePrice
print("--- Logistic Regression Summary (Without ClosePrice) ---")
X_train_nc_const = sm.add_constant(X_train_nc)
logit_model_nc = sm.Logit(y_train_nc, X_train_nc_const)
result_nc = logit_model_nc.fit(method='bfgs', disp=0)
print(result_nc.summary())
```

## Conclusion

This analysis built logistic regression models to predict competitive eBay auctions. The key finding is that while including ClosePrice improves accuracy, it creates a data leakage problem since ClosePrice is only known after an auction ends. The practical model (without ClosePrice) should be used for real-world predictions.

### Advantages of Removing ClosePrice

While the model without ClosePrice has lower accuracy, removing this variable provides several important advantages:

1. **Eliminates Data Leakage:** ClosePrice is determined by the auction outcome itself. By removing it, we ensure our model only uses information that would be available BEFORE the auction concludes, making predictions legitimate and unbiased.

2. **Enables Real-Time Predictions:** Without ClosePrice, the model can be deployed to predict competitiveness for NEW auctions as soon as they are listed. Sellers can use this to make informed decisions about their listings before any bids are placed.

3. **Honest Performance Evaluation:** The accuracy of the model without ClosePrice reflects the TRUE predictive power we can expect in practice. The inflated accuracy from including ClosePrice was misleading and would not generalize to real-world applications.

4. **Proper Causal Interpretation:** The remaining predictors (OpenPrice, Duration, Category, etc.) represent genuine factors that INFLUENCE competitiveness, rather than variables that are CAUSED BY competitiveness. This maintains the correct causal direction for prediction.

5. **Actionable Business Insights:** The model now identifies factors that sellers can actually control or consider when listing items (e.g., starting price, auction duration, listing day), providing actionable recommendations for maximizing auction competitiveness.